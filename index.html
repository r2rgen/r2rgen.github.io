
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="R2RGen: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation">
  <meta name="keywords" content="Data Generation, Imitation Learning, Visuomotor Policy, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    R2RGen
  </title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">  
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
</head>
<!-- <body onload="updateInTheWild();updateBimanual();"> -->


  
<section class="hero">
  <div class="hero-body" style="padding-top: 1.5%;">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="color:#6510ad; white-space: nowrap;">
            <span>
              R2RGen: Real-to-Real 3D Data Generation <br> for Spatially Generalized Manipulation
            </span>
          </h1>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors
            </span>
          </div>
          <br>
          <div class="button-container">
            <a href="" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="fa-light fa-code"></i>&emsp14;Code</a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" controls height="100%" width="100%">
            <source src="videos/teaser.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered" style="margin-top: -0.8em;">
        R2RGen generates <b>spatially diverse</b> manipulation demonstrations for training <b>real-world</b> policies, requiring only <b>one</b> human demonstration without simulator setup. <b>Results</b>: 1human demo + R2RGen > 25 human demos!
        </h2>
      </div>
    </div>
  </div>

  <div class="container is-max-widescreen">
  




<hr style="margin-top: 1em;">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    
    <h2 class="title is-3" style="margin-bottom: 0.5em; color:#6510ad">Abstract</h2>
    <img style="margin-bottom: 5px;" src="img/teaser.png" class="method-image" width="75%" /><br>
    <div class="content has-text-justified">
      <p>
        Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.
      </p>
    </div>
  </div>
</div>





<hr style="margin-top: 1em;">
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.5em; color:#6510ad">Methods</h2>
  </br>
  <div class="columns is-vcentered  is-centered">
    <img style="margin-bottom: 5px;" src="img/pre-process.png" class="method-image" width="35%" height="35" />
    <img style="margin-bottom: 5px;" src="img/augment-pipeline.png" class="method-image" width="55%" height="55" /><br>
  </div>
  <p class="content has-text-justified">
    <strong>Pre-processing.</strong> We design an annotation system with shape-aware source pre-processing. After processing, the 3D scene is parsed into complete objects, environment and robotâ€™s arm. The trajectory is parsed into interleaved motion and skill segments. <br>
    <strong>Pipeline of R2RGen.</strong> Given processed source demonstration, we backtrack skills and apply group-wise augmentation to maintain the spatial relationships among target objects, where a fixed object set is maintained to judge whether the augmentation is applicable. Then motion planning is performed to generate trajectories that connect adjacent skills. After augmentation, we perform camera-aware processing to make the pointclouds follow distribution of RGB-D camera. The solid arrows indicate the processing flow, while the dashed arrows indicate the updating of fixed object set.
  </p>
</div>


<br>
<hr style="margin-top: 1em;">
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.8em; color:#6510ad">Experiments</h2>
    <h3 class="title is-4" style="margin-top: 1.8em; margin-bottom: 0.8em; color:#6510ad">Source & Generated Demonstrations</h3>
    <div class="rows">


      <!-- <p class="content has-text-centered">
        <span style="letter-spacing: 0.07em;">V<span style="font-variant: small-caps; font-size: 1.2em;">i</span>L<span style="font-variant: small-caps; font-size: 1.2em;">a</span></span> effectively utilizes visual feedback in an intuitive and natural way, enabling robust closed-loop planning in dynamic environments.
      </p> -->

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>


      </div>
      <p class="content" style="margin-bottom: 1.5em; text-align: left;">
        R2RGen generates spatially diverse 3D observation-action sequences given one human demonstration. It ensures the pointclouds of objects and environment are reasonably distributed under arbitrary augmentation.
      </p>

    <h3 class="title is-4" style="margin-top: 1.8em; margin-bottom: 0.8em; color:#6510ad">Evaluation Videos</h3>
    <div class="rows">


      <!-- <p class="content has-text-centered">
        <span style="letter-spacing: 0.07em;">V<span style="font-variant: small-caps; font-size: 1.2em;">i</span>L<span style="font-variant: small-caps; font-size: 1.2em;">a</span></span> effectively utilizes visual feedback in an intuitive and natural way, enabling robust closed-loop planning in dynamic environments.
      </p> -->

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist1"
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="videos/teaser.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>

      </div>
      <p class="content" style="margin-bottom: 1.5em; text-align: left;">
        With R2RGen, it is possible to train 3D policy that can generalizes to varying objects' locations and rotations as well as robot's viewpoints. All we need is one human demonstration and real-world data augmentation, no simulator or rendering required.
      </p>
</div>
 
<br>
<hr style="margin-top: 1em;">
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.8em; color:#6510ad">Extension & Application</h2>
  <h3 class="title is-4" style="margin-bottom: 0.5em; color:#6510ad">Extension: Appearance Generalization</h3>
  <br>
  <div class="columns is-vcentered  is-centered">
    <video id="method-examples" controls  height="70%" width="70%" style="margin-left: 5px;">
      <source src="videos/teaser.mp4" type="video/mp4">
    </video>
  </div>
  <p class="content has-text-justified">
    Beyond spatial generalization, robotic manipulation tasks involve other forms of generalization, such as appearance generalization (i.e., adapting to novel object instances and environments) and task generalization. Among these, spatial generalization serves as the fundamental prerequisite for other generalization capabilities. Since this work focuses on single-task visuomotor policy learning, we investigate whether the spatial generalization enabled by R2RGen can further facilitate appearance generalization. As shown in Figure~\ref{fig:appearance}, we design a more challenging Place-Bottle task with four distinct bottle-base appearance combinations (2 bottle types x 2 base types). We observe that achieving both appearance and spatial generalization significantly increases data demand. Even with 40 human demonstrations (10 per bottle-base pair), the policy only reaches a 25% success rate. In contrast, using R2RGen, only 1 demonstration per bottle-base pair (4 in total) is needed to achieve a success rate of 43.8%, demonstrating its efficiency in handling combined generalization challenges.
  </p>
  <img style="margin-bottom: 5px;" src="img/appearance.png" class="method-image" width="60%" />
  <br>
  <br>
  <h3 class="title is-4" style="margin-bottom: 0.5em; color:#6510ad">Application: Mobile Manipulation</h3>
  <br>
  <div class="columns is-vcentered  is-centered">
    <video id="method-examples" controls  height="70%" width="70%" style="margin-left: 5px;">
      <source src="videos/teaser.mp4" type="video/mp4">
    </video>
  </div>
  <p class="content has-text-justified">
    R2RGen makes our 3D policy achieve strong spatial generalization across different viewpoints without camera calibration, so we can achieve mobile manipulation by simply combining a navigation system and a manipulation policy trained with R2RGen. Since the termination condition of navigation is relatively loose, the robot may stop at different docking point around the manipulation area, which imposes great challenges on the manipulation policy.
	According to figure below, using iDP3 trained with R2RGen, the policy successfully generalizes to different docking points with maximum distance larger than 5cm. Different from DemoGen (DP3) which requires a careful calibration of the camera pose to crop environment pointclouds, our method directly applies on raw RGB-D observations during both data generation and policy training / inference stages, which is more practical in real-world applications.
  </p>
  <img style="margin-bottom: 5px;" src="img/mobile-manipulation.png" class="method-image" width="60%" />
</div>

<br>
<br>


</section>
</div>


<script>
  document.getElementById('shuffle-video').addEventListener('click', function() {
    var taskSelect = document.getElementById('task-selection');
    var spatialSelect = document.getElementById('spatial-selection');
    randomizeSelect(taskSelect);
    randomizeSelect(spatialSelect);
    console.log("Shuffle", taskSelect.value, spatialSelect.value)
    SelectTestVideo();
  });

  function randomizeSelect(selectElement) {
    var options = selectElement.options;
    random_move = Math.random();
    var randomIndex = Math.floor(Math.random() * options.length);
    selectElement.selectedIndex = randomIndex;
  }

  function SelectTestVideo() {
    var task_name = document.getElementById("task-selection").value;
    var spatial_id = document.getElementById("spatial-selection").value;

    console.log("SelectTestVideo", task_name, spatial_id)
    var video = document.getElementById("test-video");
    video.src = "media/videos/evaluations/" + task_name + "/" + spatial_id + ".mp4";
    video.play();
  }
</script>

<script>
  document.getElementById('shuffle-obstacle').addEventListener('click', function() {
    var objectSelect = document.getElementById('object-obstacle');
    randomizeSelect(objectSelect);
    console.log("Shuffle", objectSelect.value)
    SelectObstacle();
  });

  function randomizeSelect(selectElement) {
    var options = selectElement.options;
    random_move = Math.random();
    var randomIndex = Math.floor(Math.random() * options.length);
    selectElement.selectedIndex = randomIndex;
  }

  function SelectObstacle() {
    var object_obstacle = document.getElementById("object-obstacle").value;

    console.log("SelectObstacle", object_obstacle)
    var video = document.getElementById("test-obstacle");
    video.src = "media/videos/obstacle/" + object_obstacle + ".mp4";
    video.play();
  }
</script>


<script>
  document.getElementById('shuffle-disturb').addEventListener('click', function() {
    var policySelect = document.getElementById('policy-disturb');
    var directionSelect = document.getElementById('direction-disturb');
    var numberSelect = document.getElementById('number-disturb');
    randomizeSelect(policySelect);
    randomizeSelect(directionSelect);
    randomizeSelect(numberSelect);
    console.log("Shuffle", policySelect.value, policySelect.value, numberSelect.value)
    SelectDisturb();
  });

  function randomizeSelect(selectElement) {
    var options = selectElement.options;
    random_move = Math.random();
    var randomIndex = Math.floor(Math.random() * options.length);
    selectElement.selectedIndex = randomIndex;
  }

  function SelectDisturb() {
    var policy = document.getElementById("policy-disturb").value;
    var direction = document.getElementById("direction-disturb").value;
    var number = document.getElementById("number-disturb").value;

    console.log("SelectTestVideo", policy, direction, number)
    var video = document.getElementById("test-disturb");
    video.src = "media/videos/disturb_eval/" + policy + "/" + direction + "/" + number + ".mp4";
    video.play();
  }
</script>

<style>
  .button.is-info.is-outlined:focus,
  .button.is-info.is-outlined:active {
    background-color: transparent;
    border-color: #6510ad;
    color: #6510ad;
    box-shadow: none;
  }

  .button.is-info.is-outlined:hover {
    background-color: #6510ad;
    color: #fff;
  }

  .button.is-info.is-outlined:hover .icon ion-icon,
  .button.is-info.is-outlined:hover span {
    color: #fff;
  }
</style>

</body>
</html>
